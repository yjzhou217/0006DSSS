{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9ca152a1-8b99-4195-bb08-b11b2283cde8",
      "metadata": {},
      "source": [
        "# Practical 3: Supervised learning workflow\n",
        "\n",
        "This week will introduce the supervised learning framework and key\n",
        "metrics for evaluating supervised learning models using the London Fire\n",
        "Brigade dataset.\n",
        "\n",
        "## Learning Outcomes\n",
        "\n",
        "-   You have familiarised yourself with the key concepts of supervised\n",
        "    machine learning workflow, including train-test split, cross\n",
        "    validation, and hyperparameter tuning.\n",
        "-   You are able to explain the differences between different workflows,\n",
        "    including their pros and cons.\n",
        "\n",
        "# Starting the Practical\n",
        "\n",
        "The process for every week will be the same: download the notebook to\n",
        "your `DSSS` folder (or wherever you keep your course materials), switch\n",
        "over to `JupyterLab` (which will be running in Podman/Docker) and get to\n",
        "work.\n",
        "\n",
        "If you want to save the completed notebook to your Github repo, you can\n",
        "`add`, `commit`, and `push` the notebook in Git after you download it.\n",
        "When you’re done for the day, save your changes to the file (this is\n",
        "very important!), then `add`, `commit`, and `push` your work to save the\n",
        "completed notebook.\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> Suggestions for a Better Learning Experience:\n",
        ">\n",
        "> -   **Set your operating system and software language to English**:\n",
        ">     this will make it easier to follow tutorials, search for solutions\n",
        ">     online, and understand error messages.\n",
        ">\n",
        "> -   **Save all files to a cloud storage service**: use platforms like\n",
        ">     Google Drive, OneDrive, Dropbox, or Git to ensure your work is\n",
        ">     backed up and can be restored easily when the laptop gets stolen\n",
        ">     or broken.\n",
        ">\n",
        "> -   **Avoid whitespace in file names and column names in datasets**\n",
        "\n",
        "# Revisiting London Fire Brigade Dataset\n",
        "\n",
        "This week, we will continue using the London Fire Brigade (LFB) dataset\n",
        "for supervised learning tasks. For the context of LFB data and the two\n",
        "learning tasks, please refer to Week 2 practical notebook. Briefly, we\n",
        "formulated two supervised learning tasks using the LFB dataset and have\n",
        "got some initial results:\n",
        "\n",
        "1.  *Regression*: predicting daily LFB callouts in Greater London, using\n",
        "    weather and temporal features.\n",
        "2.  *Classification*: predicting whether a fire incident is a false\n",
        "    alarm given the location available at the time of the callout, which\n",
        "    includes time of day, day of week, building type (dwelling or\n",
        "    commercial).\n",
        "\n",
        "# Predicting daily LFB callouts\n",
        "\n",
        "Remember in predicting daily LFB callouts, we used a random forest model\n",
        "and a train-test split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f8adfcd1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train R-squared: 0.918\n",
            "Test R-squared: 0.180\n"
          ]
        }
      ],
      "source": [
        "# import data from https://raw.githubusercontent.com/huanfachen/DSSS_2025/refs/heads/main/data/LFB_2023_daily_data.csv\n",
        "import pandas as pd\n",
        "df_lfb_daily = pd.read_csv(\"https://raw.githubusercontent.com/huanfachen/DSSS_2025/refs/heads/main/data/LFB_2023_daily_data.csv\")\n",
        "\n",
        "# using Random Forest to predict IncidentCount using weather, weekday, weekend, and bank holiday info\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# prepare data for modeling\n",
        "feature_cols = ['TX', 'TN', 'TG', 'SS', 'SD','RR','QQ', 'PP','HU','CC', 'IsWeekend', 'IsBankHoliday', 'weekday']\n",
        "X = df_lfb_daily[feature_cols]\n",
        "y = df_lfb_daily['IncidentCount']\n",
        "\n",
        "# one-hot encode the 'weekday' column\n",
        "X = pd.get_dummies(X, columns=['weekday'], drop_first=True)\n",
        "\n",
        "# split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# train Random Forest model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# evaluate model performance on training and testing sets\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# compute R-squared on training and testing data\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "print(f'Train R-squared: {r2_train:.3f}')\n",
        "print(f'Test R-squared: {r2_test:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a172c1e0-66d0-4043-8907-40a685c42351",
      "metadata": {},
      "source": [
        "It is obvious that the model trained is **overfitting** the training\n",
        "data, as the $R^2$ on the training and testing data is around 0.92 and\n",
        "only 0.18, respectively. Therefore, this model is not useful in\n",
        "practice, as it doesn’t generalise well to unseen data.\n",
        "\n",
        "To mitigate this issue, we can use **cross-validation** to tune the\n",
        "hyperparameters of the random forest model to reduce overfitting. The\n",
        "hyperparameters to tune include the following (see\n",
        "[link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
        "for details):\n",
        "\n",
        "-   `max_depth`: maximum depth of the tree (default at None, meaning\n",
        "    nodes are expanded until all leaves are pure or until all leaves\n",
        "    contain less tha nmin_samples_split samples)\n",
        "-   `min_samples_leaf`: minimum number of samples required to be at a\n",
        "    leaf node (default at 1)\n",
        "-   `max_features`: number of features to consider when looking for the\n",
        "    best split (default to 1.0, meaning all features are considered)\n",
        "\n",
        "We haven’t introduced random forest algorithm yet, which is the content\n",
        "of Week 4. For now, please assume that these hyperparameters can control\n",
        "the complexity of the random forest model, and tuning them can help\n",
        "reduce overfitting.\n",
        "\n",
        "## Cross-validation and grid search\n",
        "\n",
        "Cross-validation and grid search are techniques to tune hyperparameters\n",
        "and evaluate model performance more robustly.\n",
        "\n",
        "In k-fold cross-validation, the training data is split into *k* folds,\n",
        "and the model is trained on *k-1* folds and validated on the remaining\n",
        "fold. This process is repeated *k* times, with each fold used as the\n",
        "validation set once. The average performance across all folds is used to\n",
        "evaluate the model.\n",
        "\n",
        "In grid search, a set of hyperparameter values is defined, and the model\n",
        "is trained and evaluated for each combination of hyperparameters using\n",
        "cross-validation. The combination that yields the best average\n",
        "performance across all folds is selected as the optimal hyperparameter\n",
        "set.\n",
        "\n",
        "Estimating the computing time of cross-validation with grid search is\n",
        "key to planning the experiment, especially with large datasets or\n",
        "complex models. The total number of models to train is equal to the\n",
        "number of hyperparameter combinations multiplied by the number of folds\n",
        "in cross-validation. For example, if we have 3 hyperparameters with 4,\n",
        "3, and 3 possible values, respectively, and we use 5-fold\n",
        "cross-validation, the total number of models to train is\n",
        "$4 \\times 3 \\times 3 \\times 5 = 180$.\n",
        "\n",
        "The time for training a single model is estimated as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "61c669d0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training time: 0.122 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "start_time = time.time()\n",
        "model.fit(X_train, y_train)\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f'Training time: {training_time:.3f} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f53f553-aad1-4cc0-b021-bcb428750538",
      "metadata": {},
      "source": [
        "This training took around 0.28 seconds on my desktop, so the total time\n",
        "for cross-validation with grid search would be approximately\n",
        "$180 \\times 0.28 = 50.4$ seconds, less than 1 minute.\n",
        "\n",
        "We will start with defining the hyperparameters to tune and their\n",
        "possible values. The first value of `max_depth` and `min_samples_leaf`\n",
        "is their default value in `RandomForestRegressor`, respectively. See\n",
        "[link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f8a54ee8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5-fold CV to tune RandomForestRegressor\n",
        "\n",
        "# the code below is a duplicate of the above code for data preparation\n",
        "\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.metrics import r2_score\n",
        "\n",
        "# df_lfb_daily = pd.read_csv(\"https://raw.githubusercontent.com/huanfachen/DSSS_2025/refs/heads/main/data/LFB_2023_daily_data.csv\")\n",
        "\n",
        "# feature_cols = ['TX', 'TN', 'TG', 'SS', 'SD','RR','QQ', 'PP','HU','CC', 'IsWeekend', 'IsBankHoliday', 'weekday']\n",
        "# X = df_lfb_daily[feature_cols]\n",
        "# y = df_lfb_daily['IncidentCount']\n",
        "# X = pd.get_dummies(X, columns=['weekday'], drop_first=True)\n",
        "\n",
        "# # split data\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "  'max_depth': [None, 5, 10, 20],\n",
        "  'min_samples_leaf': [1, 2, 4],\n",
        "  'max_features': ['sqrt', 'log2', 0.5]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbecf5a9-2e2a-4dce-885f-3635c7e65a4f",
      "metadata": {},
      "source": [
        "We will then use `GridSearchCV` function from `sklearn.model_selection`\n",
        "to perform grid search with 5-fold cross-validation to find the optimal\n",
        "hyperparameters. This function is very handy, as it automatically\n",
        "handles the cross-validation and hyperparameter tuning. Moreover, we\n",
        "will *fit* the grid search object on the training data just like how we\n",
        "fit a model. The power of interface design.\n",
        "\n",
        "After fitting, we can access the best hyperparameters and the best\n",
        "cross-validated $R^2$ using the `best_params_` and `best_score_`\n",
        "attributes, respectively. Then, we will retrain the final model using\n",
        "the optimal hyperparameters on the entire training data and evaluate its\n",
        "performance on both the training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f939f574",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1}\n",
            "Best CV R-squared: 0.368\n",
            "CV training time: 2.935 seconds\n",
            "Train R-squared: 0.860\n",
            "Test R-squared: 0.200\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "grid = GridSearchCV(\n",
        "  estimator=RandomForestRegressor(random_state=42),\n",
        "  param_grid=param_grid,\n",
        "  cv=5,\n",
        "  scoring='r2',\n",
        "  n_jobs=-1, # n_jobs=-1 use all available cores\n",
        "  return_train_score=True\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# print best hyperparameters and best CV R-squared\n",
        "print(\"Best hyperparameters:\", grid.best_params_)\n",
        "print(f\"Best CV R-squared: {grid.best_score_:.3f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f'CV training time: {training_time:.3f} seconds')\n",
        "\n",
        "# retrain with optimal hyperparameters\n",
        "best_params = grid.best_params_\n",
        "best_model = RandomForestRegressor(random_state=42, **best_params)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# r2 on training and testing data\n",
        "train_r2 = r2_score(y_train, best_model.predict(X_train))\n",
        "print(f\"Train R-squared: {train_r2:.3f}\")\n",
        "test_r2 = r2_score(y_test, best_model.predict(X_test))\n",
        "print(f\"Test R-squared: {test_r2:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca808b6b-4cc7-4a0e-85e9-cfbcc98cd17d",
      "metadata": {},
      "source": [
        "The GridSearchCV took around 3.66 seconds on my desktop, which is much\n",
        "faster than the estimated time. This is because the `n_jobs=-1` argument\n",
        "allows parallel processing and using all processors, which speeds up the\n",
        "computation significantly.\n",
        "\n",
        "The best hyperparameters found are as follows: - `max_depth`: 10 -\n",
        "`max_features`: ‘sqrt’ - `min_samples_leaf`: 1\n",
        "\n",
        "And the $R^2$ on the training, testing, and cross-validated data are as\n",
        "follows:\n",
        "\n",
        "-   Training data: 0.860\n",
        "-   Testing data: 0.200\n",
        "-   CV: 0.368\n",
        "\n",
        "The $R^2$ on the training data has decreased from 0.92 to 0.86, while\n",
        "the $R^2$ on the testing data has increased from 0.18 to 0.20,\n",
        "indicating that the overfitting issue has been mitigated to some extent.\n",
        "Note that the cross-validated $R^2$ is higher than the testing $R^2$,\n",
        "which indicates that the model is *overfitting* during cross validation\n",
        "and the best way to evaluate the model is using the unseen testing data.\n",
        "\n",
        "# A *real* prediction task\n",
        "\n",
        "Now that we have a trained model that can predict the daily LFB\n",
        "callouts. However, this model is not really trained on the past data, as\n",
        "we use a random train-test split and a random CV. This might\n",
        "overestimate the model performance, as the model might have seen future\n",
        "data during training.\n",
        "\n",
        "In a real prediction task, we should use a temporal train-test split,\n",
        "where the training data is from the past and the testing data is from\n",
        "the future. In the next part, we will use the first 80% data (sorted by\n",
        "date) for training and the remaining data for testing.\n",
        "\n",
        "Please note that the analysis below doesn’t generate high predictive\n",
        "accuracy. Rather, this analysis serves as an example of how to implement\n",
        "temporal train-test split and temporal cross-validation in practice.\n",
        "\n",
        "## Temporal train-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "09b48281",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temporal split — Train R-squared: 0.925\n",
            "Temporal split — Test R-squared: -2.673\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# load and sort by date to respect time order\n",
        "df_lfb_daily = pd.read_csv(\"https://raw.githubusercontent.com/huanfachen/DSSS_2025/refs/heads/main/data/LFB_2023_daily_data.csv\")\n",
        "df_lfb_daily['DateOfCall'] = pd.to_datetime(df_lfb_daily['DateOfCall'])\n",
        "\n",
        "# sort by date\n",
        "df_lfb_daily = df_lfb_daily.sort_values('DateOfCall')\n",
        "\n",
        "feature_cols = ['TX', 'TN', 'TG', 'SS', 'SD','RR','QQ', 'PP','HU','CC', 'IsWeekend', 'IsBankHoliday', 'weekday']\n",
        "X = df_lfb_daily[feature_cols]\n",
        "y = df_lfb_daily['IncidentCount']\n",
        "X = pd.get_dummies(X, columns=['weekday'], drop_first=True)\n",
        "\n",
        "# temporal split: first 80% dates for training, remaining 20% for testing\n",
        "split_idx = int(len(X) * 0.8)\n",
        "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
        "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
        "\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "train_r2 = r2_score(y_train, rf.predict(X_train))\n",
        "test_r2 = r2_score(y_test, rf.predict(X_test))\n",
        "\n",
        "print(f\"Temporal split — Train R-squared: {train_r2:.3f}\")\n",
        "print(f\"Temporal split — Test R-squared: {test_r2:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d7f71a1-2c33-425b-b968-351599861ecb",
      "metadata": {},
      "source": [
        "Not surprisingly, the model is overfitting the training data and does\n",
        "poorly on the testing data, with $R^2$ of 0.925 and -2.673 on the\n",
        "training and testing data, respectively. This is because we included\n",
        "only one-year data, and the training data and testing data are from\n",
        "different seasons. If we included multiple years of data, the model\n",
        "would perform much better on the testing data.\n",
        "\n",
        "## Temporal cross-validation with grid search\n",
        "\n",
        "Below, we will show how to use temporal cross-validation to tune the\n",
        "hyperparameters of the random forest model. We will use\n",
        "`TimeSeriesSplit` from `sklearn.model_selection` to perform temporal\n",
        "cross-validation, which ensures that the training data is always before\n",
        "the validation data in time, thus avoiding data leakage.\n",
        "\n",
        "<img src=\"images/time_series_cv.png\" alt=\"Time series cross-validation\" style=\"max-width:85%;\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6ab11fe3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters (temporal CV): {'max_depth': 20, 'max_features': 0.5, 'min_samples_leaf': 1}\n",
            "Best temporal CV R-squared: -0.269\n",
            "Temporal split — Train R-squared with tuned params: 0.928\n",
            "Temporal split — Test R-squared with tuned params: -2.687\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "\n",
        "# use only the training window for temporal CV to avoid leakage\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "param_grid = {\n",
        "  'max_depth': [None, 5, 10, 20],\n",
        "  'min_samples_leaf': [1, 2, 4],\n",
        "  'max_features': ['sqrt', 'log2', 0.5]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "  estimator=RandomForestRegressor(random_state=42),\n",
        "  param_grid=param_grid,\n",
        "  cv=tscv,\n",
        "  scoring='r2',\n",
        "  n_jobs=-1,\n",
        "  return_train_score=True\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best hyperparameters (temporal CV):\", grid.best_params_)\n",
        "print(f\"Best temporal CV R-squared: {grid.best_score_:.3f}\")\n",
        "\n",
        "# retrain on full training window with best params, evaluate on held-out test window\n",
        "best_params = grid.best_params_\n",
        "best_model = RandomForestRegressor(random_state=42, **best_params)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# r2 on training and testing data\n",
        "train_r2_temporal = r2_score(y_train, best_model.predict(X_train))\n",
        "print(f\"Temporal split — Train R-squared with tuned params: {train_r2_temporal:.3f}\")\n",
        "\n",
        "test_r2_temporal = r2_score(y_test, best_model.predict(X_test))\n",
        "print(f\"Temporal split — Test R-squared with tuned params: {test_r2_temporal:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7a11577-17d0-44c2-9111-f8f01c223144",
      "metadata": {},
      "source": [
        "The $R^2$ on the training, testing, and cross-validated data are as\n",
        "follows:\n",
        "\n",
        "-   Training data: 0.928\n",
        "-   Testing data: -2.687\n",
        "-   CV: -0.269\n",
        "\n",
        "The hyperparameter tuning doesn’t improve the model performance on the\n",
        "testing data. This is due to insufficient data size and limited\n",
        "features. In practice, we would need more historical data (e.g. multiple\n",
        "years) and more potential relevant features (e.g. socio-economic, land\n",
        "use, historical fire incident density, remote sensing) to build a robust\n",
        "model for predicting daily LFB callouts.\n",
        "\n",
        "Can you give it a try?\n",
        "\n",
        "# Future directions\n",
        "\n",
        "That’s mostly for this practical. We have demonstrated how to predict\n",
        "LFB daily callouts withcross validation and hyperparameter tuning using\n",
        "grid search in both random and temporal data splits. The similar\n",
        "workflow can be applied to the classification task of predicting false\n",
        "alarms in fire incidents, and we will demonstrate this in later\n",
        "practicals.\n",
        "\n",
        "## References and recommendations:\n",
        "\n",
        "1.  There is not much (geospatial) machine learning research on London\n",
        "    Fire Brigade datasets in academia. The [blog by GTH\n",
        "    Consulting](https://www.gthconsulting.co.uk/blog-list) provides some\n",
        "    interesting articles on fire service data analysis in the UK, which\n",
        "    receive lots of comments on LinkedIn\n",
        "    (e.g. [this](https://www.linkedin.com/posts/gth-consulting_fire-risk-assessment-activity-7049279279279279360-7v5A))."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
